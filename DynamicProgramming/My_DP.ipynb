{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m39z1qdQpahm"
      },
      "source": [
        "# Vanilla Deep Q-Learning\n",
        "---\n",
        "In this notebook, you will implement Dynamic Programming algorithm.\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PrzemekSekula/ReinforcementLearningClasses/blob/main/DynamicProgramming/DP_empty.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQcAat7Zpahq"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !wget https://raw.githubusercontent.com/PrzemekSekula/ReinforcementLearningClasses/main/DynamicProgramming/helper.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEQ0mBwQpahs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from helper import State, Simulator\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS34hEOYpahs"
      },
      "source": [
        "# World description\n",
        "### Actions\n",
        "Our world is a grid world. An agent can move the world travelling in four main directions, so four actions are possible:\n",
        "- 0 (up)\n",
        "- 1 (right)\n",
        "- 2 (down)\n",
        "- 3 (left)\n",
        "\n",
        "Each action moves us to the corresponding state. If it is impossible to be moved, we are staying in the same state. There is also a terminal state (marked red). Reaching a terminal state ends the episode.\n",
        "\n",
        "### States\n",
        "A state is a location of the agent. A standard way to describe the state is by using an instance of a `State` class. Such an object has two properties: `state.row` and `state.col` that are describing the position of the agent. Both rows and cols are counted from `0`, so the upper left corner corresponds to `(0, 0)` state. Another way of describing a state is by using a tuple `(row, col)`. Such a format is also accepted by methods implemented in a simulator.\n",
        "\n",
        "### Rewards\n",
        "For each move a negative reward `Reward = -1` is granted. Aditionally, for entering each state a reward associated with this state is granted.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T4Xiwgzpaht"
      },
      "source": [
        "# Simulator Description\n",
        "A main goals of a simulator are as follows:\n",
        "- store the data about the world\n",
        "- store the current policy\n",
        "- store the current value function\n",
        "- facilitate RL-related operations\n",
        "\n",
        "#### Properties:\n",
        "- `world` - numpy.array with the world. The numbers correspond to the rewards for reaching each state\n",
        "- `policy` - numpy.array with policy. Policy is always deterministic. The numbers represent specific actions: 0 (up), 1 (right), 2 (down), 3 (left)\n",
        "- `values` - numpy.array with the state-value function for each state\n",
        "- `reward` - aditional reward granted for performing each action\n",
        "- `terminal` - a terminal state. It is an instance of the `State` class.\n",
        "\n",
        "#### Methods:\n",
        "- `move` - Returns a state that is the result of an action.\n",
        "- `getReward` - Returns the reward for entering a specific state (location).\n",
        "- `getValue` - Returns a Value function for a determined state (location).\n",
        "- `getPolicy` - Returns a policy for a determined state (location).\n",
        "- `setValue` - Sets a Value function for a determined state (location).\n",
        "- `setPolicy` - Sets a policy for a determined state (location).\n",
        "- `plot` - Visualizes the world, value function and policy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41sbZ2QGpaht"
      },
      "outputs": [],
      "source": [
        "world = np.array([\n",
        "    [ 0,    0,   0,  0],\n",
        "    [-10, -10, -15,  0],\n",
        "    [ 0,   0,   0,   0],\n",
        "    [ 0, -10, -10, -10],\n",
        "    [ 0,   0,   0,   0],\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "sim = Simulator(\n",
        "    world = world, # Our World\n",
        "    terminal = [x-1 for x in world.shape], # t. state in lower right corner\n",
        "    reward = -1 # Reward for each step\n",
        "    )\n",
        "\n",
        "sim.policy = 3 + sim.world * 0\n",
        "sim.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4CT2OV9pahu"
      },
      "source": [
        "#### Reward, Value, Policy\n",
        "We can communicate with world, reading Rewards, Value functions and\n",
        "Let's see how it works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2k1vqf7lpahu"
      },
      "outputs": [],
      "source": [
        "#Let's start with state (location) 1, 2 (row nr 1, column nr 2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLz1Ol5spahv"
      },
      "source": [
        "##### Setting values and policies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imJmdAJrpahv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG9Vr3_opahw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F18iwD5tpahw"
      },
      "source": [
        "#### Using `State` class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zo5v8bZdpahw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czJwQeOVpahw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCiuGUKPpahw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PojLu2dOpahx"
      },
      "source": [
        "#### Using `simulator.move()` method\n",
        "There are two ways of using `simulator.move()` method. In the first version, you provide two arguments:\n",
        "- `state` - the starting location that you want to move from. It may be either an instance of a `State` class or a tupple with (row, col) coordinates\n",
        "- `action` - action to be performed. One of: 0 (up), 1 (right), 2 (down), 3 (left)\n",
        "\n",
        "The method returns a destination state (an instance of the `State()` class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxD7fmdCpahx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8VOIlSqpahx"
      },
      "source": [
        "In the second version you should just provide a starting location (state) that you want to move from. The action is selected from the current policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JA3Om6H6pahx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0p1P7a5pahx"
      },
      "source": [
        "# TO DO\n",
        "Create a code that uses one of DP algorithms to find an optimal policy for the given world.\n",
        "To pass the assignment you have to:\n",
        "- show that your code works\n",
        "- understand the code\n",
        "- understand the theory behind the code\n",
        "- understand the general idea of dynamic programming.\n",
        "\n",
        "*Note 1: It is not necessary to use the simulator, but you should at least consider it. It will make your life much easier.*\n",
        "\n",
        "*Note 2: You may use any discount rate you wish, but I recommend you to discount your rewards (use something < 1, eg. 0.9)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXV-i2Jvpahx"
      },
      "outputs": [],
      "source": [
        "world = -np.array([\n",
        "    [ 0,  0,  0,  0,  0,  0],\n",
        "    [10, 10, 11, 10, 10,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0],\n",
        "    [ 0, 10, 10,  4, 10, 10],\n",
        "    [ 0,  0,  0,  0,  0,  0],\n",
        "    [10, 10, 10, 10, 10,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0],\n",
        "    [ 0, 10, 10, 10, 10, 10],\n",
        "    [ 0,  0, 0,   0,  0,  0],\n",
        "])\n",
        "\n",
        "\n",
        "sim = Simulator(\n",
        "    world = world, # Our World\n",
        "    terminal = [x-1 for x in world.shape], # t. state in lower right corner\n",
        "    reward = -1 # Reward for each step\n",
        "    )\n",
        "\n",
        "sim.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4v6tW6BNpahy"
      },
      "outputs": [],
      "source": [
        "gamma = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msUHgrjFpahy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}